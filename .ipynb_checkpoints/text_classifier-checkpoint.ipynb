{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abusive Language Detection Model\n",
    "By Ezra Abah\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "<ol>\n",
    "    <li>Problem Definition</li>\n",
    "<li>Data Discovery\n",
    "    <ul>\n",
    "        <li> About Data </li>\n",
    "        <li> Data Exploration </li>\n",
    "        <li> Text Cleaning</li>\n",
    "    </ul>\n",
    "</li>\n",
    "<li> Pre-processing:Onehot encoding and word embedding\n",
    "    <ul>\n",
    "        <li> Label - Onehot Encoding </li>\n",
    "        <li> Text - Tokenization </li>\n",
    "        <li> GloVe word Embedding</li>\n",
    "    </ul>\n",
    "</li>\n",
    "\n",
    "<li>Model Development\n",
    "    <ul>\n",
    "        <li> Model architecture</li>\n",
    "        <li> Model training</li>\n",
    "     </ul>\n",
    "</li>\n",
    "<li>Model evaluation and report</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Definition\n",
    "The objective of this project is to  develop and evaluate abusive language detection models for the given dataset. To do this, this project sets off in section 2. by importing, exploring and cleaning the data. In section 3, the data is preprocessed for modelling. The modelling section, section 4, starts by creating a baseline model afterwhich subsequent models are trained, varying hyperparameters. Finally in section 5, the trained models are evaluated using accuracy, precision, F-1 score and Recall and the optimum hyperparameter is selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 About Data\n",
    "This dataset, \"agr_en_train.csv\" is a Comma Separated Variable (.csv) file which contains about 12,000 observations and 3 columns namely: unique_id,text and aggression-level.\n",
    "\n",
    "<ol>\n",
    "    <li><b>unique_id</b>: This column contains unique id's for the individual observations</li>\n",
    "    <li><b>text</b>: The text column contains texts collected from social media.</li>\n",
    "    <li> <b>aggression-level</b>: This measures the level of aggression in the text. There are three aggression levels:\n",
    "<ul>\n",
    "        <li>'Overtly Aggressive’ denoted as OAG </li>\n",
    "        <li>‘Covertly Aggressive’ denoted as CAG </li>\n",
    "        <li>‘Non-aggressive’ denoted as NAG</ul></li>\n",
    "    \n",
    "</ol>\n",
    "\n",
    "\n",
    "\n",
    "The full dataset (both the train and test sets) is licensed under Creative Commons Non-Commercial Share-Alike 4.0 licence CC-BY-NC-SA 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"agr_en_train.csv\", names=[\"index\",\"text\",\"aggression_level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>aggression_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook_corpus_msr_1723796</td>\n",
       "      <td>Well said sonu..you have courage to stand agai...</td>\n",
       "      <td>OAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook_corpus_msr_466073</td>\n",
       "      <td>Most of Private Banks ATM's Like HDFC, ICICI e...</td>\n",
       "      <td>NAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook_corpus_msr_1493901</td>\n",
       "      <td>Now question is, Pakistan will adhere to this?</td>\n",
       "      <td>OAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook_corpus_msr_405512</td>\n",
       "      <td>Pakistan is comprised of fake muslims who does...</td>\n",
       "      <td>OAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook_corpus_msr_1521685</td>\n",
       "      <td>??we r against cow slaughter,so of course it w...</td>\n",
       "      <td>NAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>facebook_corpus_msr_462570</td>\n",
       "      <td>Wondering why Educated Ambassador is strugglin...</td>\n",
       "      <td>CAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>facebook_corpus_msr_465051</td>\n",
       "      <td>How does inflation react to all the after shoc...</td>\n",
       "      <td>NAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>facebook_corpus_msr_450994</td>\n",
       "      <td>Not good job.....this guis creating a problem ...</td>\n",
       "      <td>CAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>facebook_corpus_msr_326287</td>\n",
       "      <td>This is a false news Indian media is simply mi...</td>\n",
       "      <td>NAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>facebook_corpus_msr_430450</td>\n",
       "      <td>no permanent foes, no permanent friends. inter...</td>\n",
       "      <td>NAG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         index  \\\n",
       "0  facebook_corpus_msr_1723796   \n",
       "1   facebook_corpus_msr_466073   \n",
       "2  facebook_corpus_msr_1493901   \n",
       "3   facebook_corpus_msr_405512   \n",
       "4  facebook_corpus_msr_1521685   \n",
       "5   facebook_corpus_msr_462570   \n",
       "6   facebook_corpus_msr_465051   \n",
       "7   facebook_corpus_msr_450994   \n",
       "8   facebook_corpus_msr_326287   \n",
       "9   facebook_corpus_msr_430450   \n",
       "\n",
       "                                                text aggression_level  \n",
       "0  Well said sonu..you have courage to stand agai...              OAG  \n",
       "1  Most of Private Banks ATM's Like HDFC, ICICI e...              NAG  \n",
       "2     Now question is, Pakistan will adhere to this?              OAG  \n",
       "3  Pakistan is comprised of fake muslims who does...              OAG  \n",
       "4  ??we r against cow slaughter,so of course it w...              NAG  \n",
       "5  Wondering why Educated Ambassador is strugglin...              CAG  \n",
       "6  How does inflation react to all the after shoc...              NAG  \n",
       "7  Not good job.....this guis creating a problem ...              CAG  \n",
       "8  This is a false news Indian media is simply mi...              NAG  \n",
       "9  no permanent foes, no permanent friends. inter...              NAG  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view first 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of this data set is (11999, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aggression_level</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CAG</th>\n",
       "      <td>4240</td>\n",
       "      <td>4240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAG</th>\n",
       "      <td>5051</td>\n",
       "      <td>5051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAG</th>\n",
       "      <td>2708</td>\n",
       "      <td>2708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  index  text\n",
       "aggression_level             \n",
       "CAG                4240  4240\n",
       "NAG                5051  5051\n",
       "OAG                2708  2708"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observe size of dataset\n",
    "shape = df.shape\n",
    "print(\"The size of this data set is \"+ str(shape))\n",
    "df.groupby('aggression_level').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is large and will require a lot of time and computational power. To save time and memory, 3000 random sample data points are selected for this project. To equally represent each class 1000 data points was used from each aggressive level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of this data set is (3000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aggression_level</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CAG</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAG</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAG</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  index  text\n",
       "aggression_level             \n",
       "CAG                1000  1000\n",
       "NAG                1000  1000\n",
       "OAG                1000  1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample of 3000 datapoints selected and stored as a dataframe named 'data'\n",
    "\n",
    "#select 1000 datapoints from each category\n",
    "data_CAG=df[df['aggression_level'] == \"CAG\"].sample(n=1000, replace=False, random_state=5)\n",
    "data_NAG=df[df['aggression_level'] == \"NAG\"].sample(n=1000, replace=False, random_state=5)\n",
    "data_OAG=df[df['aggression_level'] == \"OAG\"].sample(n=1000, replace=False, random_state=5)\n",
    "\n",
    "#Joining all data sets into one\n",
    "data=pd.concat([data_CAG,data_NAG,data_OAG],sort=False) #Join into one dataset \"data\"\n",
    "\n",
    "data.reset_index(drop=True,inplace=True) #used to reset the index\n",
    "print(\"The size of this data set is \"+ str(data.shape))\n",
    "data.groupby('aggression_level').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index               object\n",
      "text                object\n",
      "aggression_level    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#view datatypes of individual columns\n",
    "\n",
    "types = data.dtypes\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index               0\n",
       "text                0\n",
       "aggression_level    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for empty cells\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest text contains 716 word(s).\n"
     ]
    }
   ],
   "source": [
    "# Check for the lenght of the longest text\n",
    "maxLen = len(max(data['text'], key=len).split(\" \"))\n",
    "print(\"The longest text contains \"+ str(maxLen)+\" word(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text contains 1 word(s).\n"
     ]
    }
   ],
   "source": [
    "# Check for the lenght of the shortest text\n",
    "minLen = len(min(data['text'], key=len).split())\n",
    "print(\"The shortest text contains \"+ str(minLen)+\" word(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    When they are crossing the lines over humanity...\n",
       "1    Demonetization was a good step by BJP but not ...\n",
       "2    who will take a right decision when a common p...\n",
       "3    In 2015 the educated people voted for educated...\n",
       "4    Still waiting for Rahul Gandhis grand expose o...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Punctuation\n",
    "\n",
    "data['text'] = data['text'].str.replace('[^\\w\\s]','')\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    when they are crossing the lines over humanity...\n",
       "1    demonetization was a good step by bjp but not ...\n",
       "2    who will take a right decision when a common p...\n",
       "3    in 2015 the educated people voted for educated...\n",
       "4    still waiting for rahul gandhis grand expose o...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change everything to Lowercase\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    crossing lines humanity makes scarecrow way hu...\n",
       "1            demonetization good step bjp planned well\n",
       "2    take right decision common people purchase dru...\n",
       "3    2015 educated people voted educated minister i...\n",
       "4    still waiting rahul gandhis grand expose pm sp...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove stopwords using nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data['text'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    crossing line humanity make scarecrow way huma...\n",
       "1            demonetization good step bjp planned well\n",
       "2    take right decision common people purchase dru...\n",
       "3    2015 educated people voted educated minister i...\n",
       "4    still waiting rahul gandhi grand expose pm spe...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from textblob import Word\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Data Pre-processing\n",
    "To pre-process the data for modelling, <br>\n",
    "<ul>\n",
    "    <li>Aggression levels will be encoded using one hot encoding</li>\n",
    "    <li>Texts will be tokenized, padded and individual words will be embedded</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAG' 'CAG' 'CAG' ... 'OAG' 'OAG' 'OAG']\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Encode aggression levels using sklearn's onehot encoder \n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "values = array(data['aggression_level']) #save aggression levels as an array\n",
    "print(values)\n",
    "\n",
    "# encode with integers\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "# encode using onehot\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "levels_onehot = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(levels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store the reviews and ecoded levels in two arrays as follows:\n",
    "texts = data['text'].values\n",
    "levels = levels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and testing data. 20% of data is used for testing and seed of 5 is assigned\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_test, y_train, y_test = train_test_split(texts, levels, test_size=0.20,random_state=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and pad text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doesnt befits man started career performing jagrata used sing whole night loudspeaker raise question azaan last 1 3 min also would first time would heard fajr azaan delhi clever man know could also rewarded like akshay kumar future involves religious debate\n",
      "[162, 48, 418, 988, 1745, 1746, 211, 672, 238, 765, 268, 1167, 203, 313, 212, 136, 177, 853, 15, 49, 40, 10, 49, 542, 313, 93, 1747, 48, 17, 213, 15, 6, 446, 269, 90, 766]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8711"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize texts using keras tokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#define tokeniser\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "\n",
    "#Use tokenization only on the training data\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(text_train)\n",
    "X_test = tokenizer.texts_to_sequences(text_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(text_train[0])\n",
    "print(X_train[0])\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxLen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text word embedding\n",
    "The embedding layer used is a 100d pretrained GloVe embedding trained on a twitter corpus of 27 Billion words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove_data/glove.twitter.27B/glove.twitter.27B.100d.txt',encoding=\"utf8\") # gotten from https://nlp.stanford.edu/projects/glove/\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype=np.float32)\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training file\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected architecture of the developed models was adapted from Yoon Kim's Study <i>Convolutional Neural Networks for Sentence Classification </i>. Using this approach, an end to end neural network is used such that the embedding matrix enters a single convolutional layer and after filtering is sent to a max pooling layer to reduce the features. The reduced vector is finally sent to a dropout and softmax.<br>\n",
    "This architecture was selected because the study showed that this simple CNN with one layer of convolution performs remarkably well even with little tunning.\n",
    "Tuning will be carried out manually by training and evaluating Four(4) models. In these models, all hyperparameters are held constant while the number of filters used is varied. For all models, validation set used is 0.1 of the training set. also, 10 epochs were used and a batch size of 50 was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from keras import layers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define CNN text classifier\n",
    "def conv_classifier(conv_filters):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxLen,trainable=True))\n",
    "    model.add(layers.Conv1D(conv_filters, 3, activation='relu')) #Kernel size=3, conv_filters will be varied\n",
    "    model.add(layers.GlobalMaxPooling1D()) # done to reduce the dimensionality of the features \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='Adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model 1\n",
    "In this model, the values of hyperparameters used are obtained from Yoon kim, 2014. They were chosen because based on that study, these were the optimum hyperparameters found after carrying out a grid search.\n",
    "<br>\n",
    "<ul>\n",
    "    <li>Transfer function: rectified linear(ReLU)</li>\n",
    "    <li>Kernel size: 3 (3 was used although kim used [3,4,5] which takes very long to run)</li> \n",
    "    <li>Number of filters: 100</li>\n",
    "    <li>Dropout rate: 0.5</li>\n",
    "    <li>Batch Size: 50</li>\n",
    "    <li>Optimizer: Adadelta</li>\n",
    "    <li>Final activation: Softmax</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 716, 100)          871100    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 714, 100)          30100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 901,503\n",
      "Trainable params: 901,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2160 samples, validate on 240 samples\n",
      "Epoch 1/10\n",
      "2160/2160 [==============================] - 6s 3ms/step - loss: 0.7208 - accuracy: 0.6227 - val_loss: 0.6306 - val_accuracy: 0.6722\n",
      "Epoch 2/10\n",
      "2160/2160 [==============================] - 6s 3ms/step - loss: 0.6323 - accuracy: 0.6711 - val_loss: 0.6139 - val_accuracy: 0.6681\n",
      "Epoch 3/10\n",
      "2160/2160 [==============================] - 6s 3ms/step - loss: 0.5736 - accuracy: 0.7062 - val_loss: 0.6064 - val_accuracy: 0.6736\n",
      "Epoch 4/10\n",
      "2160/2160 [==============================] - 6s 3ms/step - loss: 0.5459 - accuracy: 0.7238 - val_loss: 0.6199 - val_accuracy: 0.6625\n",
      "Epoch 5/10\n",
      "2160/2160 [==============================] - 6s 3ms/step - loss: 0.5140 - accuracy: 0.7461 - val_loss: 0.6065 - val_accuracy: 0.6694\n",
      "Epoch 6/10\n",
      "2160/2160 [==============================] - 7s 3ms/step - loss: 0.4981 - accuracy: 0.7579 - val_loss: 0.6000 - val_accuracy: 0.6875\n",
      "Epoch 7/10\n",
      "2160/2160 [==============================] - 6s 3ms/step - loss: 0.4766 - accuracy: 0.7758 - val_loss: 0.5997 - val_accuracy: 0.6778\n",
      "Epoch 8/10\n",
      "2160/2160 [==============================] - 7s 3ms/step - loss: 0.4577 - accuracy: 0.7835 - val_loss: 0.5984 - val_accuracy: 0.6861\n",
      "Epoch 9/10\n",
      "2160/2160 [==============================] - 6s 3ms/step - loss: 0.4405 - accuracy: 0.8006 - val_loss: 0.6147 - val_accuracy: 0.6653\n",
      "Epoch 10/10\n",
      "2160/2160 [==============================] - 6s 3ms/step - loss: 0.4214 - accuracy: 0.8134 - val_loss: 0.6330 - val_accuracy: 0.6597\n"
     ]
    }
   ],
   "source": [
    "#train model_1, filter=100.\n",
    "model_1 = conv_classifier(100)\n",
    "training = model_1.fit(X_train, y_train, epochs=10, verbose=True, validation_split = 0.1, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model 2\n",
    "In this model, hyperparameters used are as used in Model 1 however, number of filters is increased to 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 716, 100)          871100    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 714, 150)          45150     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 916,703\n",
      "Trainable params: 916,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2160 samples, validate on 240 samples\n",
      "Epoch 1/10\n",
      "2160/2160 [==============================] - 8s 4ms/step - loss: 0.7130 - accuracy: 0.6282 - val_loss: 0.6141 - val_accuracy: 0.6542\n",
      "Epoch 2/10\n",
      "2160/2160 [==============================] - 9s 4ms/step - loss: 0.6035 - accuracy: 0.6878 - val_loss: 0.6294 - val_accuracy: 0.6597\n",
      "Epoch 3/10\n",
      "2160/2160 [==============================] - 9s 4ms/step - loss: 0.5556 - accuracy: 0.7168 - val_loss: 0.6135 - val_accuracy: 0.6597\n",
      "Epoch 4/10\n",
      "2160/2160 [==============================] - 9s 4ms/step - loss: 0.5208 - accuracy: 0.7418 - val_loss: 0.6011 - val_accuracy: 0.6694\n",
      "Epoch 5/10\n",
      "2160/2160 [==============================] - 8s 4ms/step - loss: 0.5037 - accuracy: 0.7582 - val_loss: 0.6075 - val_accuracy: 0.6694\n",
      "Epoch 6/10\n",
      "2160/2160 [==============================] - 9s 4ms/step - loss: 0.4708 - accuracy: 0.7816 - val_loss: 0.6057 - val_accuracy: 0.6806\n",
      "Epoch 7/10\n",
      "2160/2160 [==============================] - 9s 4ms/step - loss: 0.4525 - accuracy: 0.7941 - val_loss: 0.6159 - val_accuracy: 0.6611\n",
      "Epoch 8/10\n",
      "2160/2160 [==============================] - 9s 4ms/step - loss: 0.4221 - accuracy: 0.8137 - val_loss: 0.6181 - val_accuracy: 0.6708\n",
      "Epoch 9/10\n",
      "2160/2160 [==============================] - 8s 4ms/step - loss: 0.4005 - accuracy: 0.8301 - val_loss: 0.6158 - val_accuracy: 0.6819\n",
      "Epoch 10/10\n",
      "2160/2160 [==============================] - 9s 4ms/step - loss: 0.3926 - accuracy: 0.8290 - val_loss: 0.6577 - val_accuracy: 0.6500\n"
     ]
    }
   ],
   "source": [
    "#train model_2, filters=150\n",
    "model_2 = conv_classifier(150)\n",
    "training = model_2.fit(X_train, y_train, epochs=10, verbose=True, validation_split = 0.1, batch_size=50)\n",
    "#details about the model: https://keras.io/models/model/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model 3\n",
    "In this model, hyperparameters used are as used in Model 1 however, number of filters is increased to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 716, 100)          871100    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 714, 200)          60200     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 931,903\n",
      "Trainable params: 931,903\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2160 samples, validate on 240 samples\n",
      "Epoch 1/10\n",
      "2160/2160 [==============================] - 11s 5ms/step - loss: 0.7202 - accuracy: 0.6252 - val_loss: 0.5975 - val_accuracy: 0.6736\n",
      "Epoch 2/10\n",
      "2160/2160 [==============================] - 10s 5ms/step - loss: 0.5997 - accuracy: 0.6912 - val_loss: 0.5944 - val_accuracy: 0.6750\n",
      "Epoch 3/10\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.5536 - accuracy: 0.7193 - val_loss: 0.6292 - val_accuracy: 0.6653\n",
      "Epoch 4/10\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.5107 - accuracy: 0.7489 - val_loss: 0.5926 - val_accuracy: 0.6806\n",
      "Epoch 5/10\n",
      "2160/2160 [==============================] - 11s 5ms/step - loss: 0.4747 - accuracy: 0.7742 - val_loss: 0.5955 - val_accuracy: 0.6778\n",
      "Epoch 6/10\n",
      "2160/2160 [==============================] - 11s 5ms/step - loss: 0.4557 - accuracy: 0.7892 - val_loss: 0.5998 - val_accuracy: 0.6847\n",
      "Epoch 7/10\n",
      "2160/2160 [==============================] - 11s 5ms/step - loss: 0.4308 - accuracy: 0.8065 - val_loss: 0.6051 - val_accuracy: 0.6847\n",
      "Epoch 8/10\n",
      "2160/2160 [==============================] - 11s 5ms/step - loss: 0.4084 - accuracy: 0.8216 - val_loss: 0.6162 - val_accuracy: 0.6833\n",
      "Epoch 9/10\n",
      "2160/2160 [==============================] - 11s 5ms/step - loss: 0.3793 - accuracy: 0.8384 - val_loss: 0.6460 - val_accuracy: 0.6611\n",
      "Epoch 10/10\n",
      "2160/2160 [==============================] - 11s 5ms/step - loss: 0.3626 - accuracy: 0.8528 - val_loss: 0.6202 - val_accuracy: 0.6792\n"
     ]
    }
   ],
   "source": [
    "#train model_3, filters=200\n",
    "model_3 = conv_classifier(200)\n",
    "training = model_3.fit(X_train, y_train, epochs=10, verbose=True, validation_split = 0.1, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model 4\n",
    "In this model, hyperparameters used are as used in Model 1 however, number of filters is increased to 250 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 716, 100)          871100    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 714, 250)          75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 753       \n",
      "=================================================================\n",
      "Total params: 947,103\n",
      "Trainable params: 947,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2160 samples, validate on 240 samples\n",
      "Epoch 1/10\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 0.6848 - accuracy: 0.6389 - val_loss: 0.6792 - val_accuracy: 0.6292\n",
      "Epoch 2/10\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 0.5848 - accuracy: 0.6961 - val_loss: 0.6544 - val_accuracy: 0.6472\n",
      "Epoch 3/10\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.5324 - accuracy: 0.7349 - val_loss: 0.6836 - val_accuracy: 0.6403\n",
      "Epoch 4/10\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.4983 - accuracy: 0.7617 - val_loss: 0.6105 - val_accuracy: 0.6903\n",
      "Epoch 5/10\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.4593 - accuracy: 0.7863 - val_loss: 0.6453 - val_accuracy: 0.6694\n",
      "Epoch 6/10\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.4265 - accuracy: 0.8103 - val_loss: 0.6085 - val_accuracy: 0.6681\n",
      "Epoch 7/10\n",
      "2160/2160 [==============================] - 13s 6ms/step - loss: 0.3979 - accuracy: 0.8235 - val_loss: 0.6147 - val_accuracy: 0.6722\n",
      "Epoch 8/10\n",
      "2160/2160 [==============================] - 16s 7ms/step - loss: 0.3793 - accuracy: 0.8384 - val_loss: 0.6252 - val_accuracy: 0.6778\n",
      "Epoch 9/10\n",
      "2160/2160 [==============================] - 15s 7ms/step - loss: 0.3463 - accuracy: 0.8608 - val_loss: 0.6902 - val_accuracy: 0.6569\n",
      "Epoch 10/10\n",
      "2160/2160 [==============================] - 14s 6ms/step - loss: 0.3257 - accuracy: 0.8698 - val_loss: 0.6544 - val_accuracy: 0.6597\n"
     ]
    }
   ],
   "source": [
    "#train model_4 , filters=250\n",
    "model_4 = conv_classifier(250)\n",
    "training = model_4.fit(X_train, y_train, epochs=10, verbose=True, validation_split = 0.1, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction made by the models is an array of probalities of each class. To invert that array into a single integer representation, np.argmax is used to return the indices of the maximum values along each array. This is illustrated below using model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13942306, 0.04752779, 0.8130492 ],\n",
       "       [0.2528524 , 0.01347537, 0.7336722 ],\n",
       "       [0.13010319, 0.02773136, 0.84216547],\n",
       "       ...,\n",
       "       [0.24501248, 0.5067857 , 0.2482018 ],\n",
       "       [0.28610128, 0.49950546, 0.21439318],\n",
       "       [0.1859524 , 0.69867915, 0.11536851]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2,\n",
       "       2, 0, 1, 1, 1, 0, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 0, 0, 2, 2, 1, 2,\n",
       "       1, 1, 1, 1, 1, 2, 1, 2, 2, 0, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1,\n",
       "       2, 1, 0, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 0, 1, 2, 1, 2, 1, 1, 2, 1,\n",
       "       1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 2, 0, 2, 2, 1, 1, 1, 0,\n",
       "       2, 0, 2, 1, 1, 2, 2, 2, 1, 1, 1, 0, 2, 1, 1, 2, 1, 2, 0, 2, 1, 2,\n",
       "       2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 0, 0, 2, 2, 1, 1, 0, 2, 1, 2, 1, 1,\n",
       "       0, 1, 1, 2, 2, 0, 0, 2, 1, 2, 2, 0, 1, 2, 1, 0, 2, 2, 2, 2, 2, 2,\n",
       "       0, 1, 1, 2, 1, 0, 2, 1, 2, 2, 1, 0, 2, 2, 0, 2, 0, 2, 1, 1, 1, 1,\n",
       "       2, 1, 2, 0, 2, 1, 1, 2, 1, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1,\n",
       "       2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 2, 1, 0, 1, 2, 1, 2, 1,\n",
       "       1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 0, 0, 1,\n",
       "       2, 2, 1, 2, 2, 2, 1, 0, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 0, 2,\n",
       "       1, 2, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 0, 0, 2, 0, 2, 2, 1, 0, 1, 1,\n",
       "       0, 2, 0, 2, 1, 2, 2, 2, 1, 1, 2, 1, 0, 2, 1, 1, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2,\n",
       "       1, 2, 2, 0, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 0, 0, 1, 2, 1, 1, 2, 1,\n",
       "       1, 2, 2, 2, 2, 0, 0, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 0, 1,\n",
       "       1, 1, 2, 2, 1, 2, 2, 1, 1, 0, 0, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2,\n",
       "       1, 2, 2, 0, 2, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 0, 1, 2, 2, 1, 1, 1,\n",
       "       2, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2,\n",
       "       2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 1, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2,\n",
       "       1, 1, 1, 2, 1, 0, 0, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1,\n",
       "       1, 1, 0, 2, 2, 2, 0, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2,\n",
       "       2, 1, 2, 1, 2, 2, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1,\n",
       "       1, 2, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model_1.predict(X_test), axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Model Evaluation\n",
    "In this section, the quality of models developed will be evaluated using the following metrics:\n",
    "<ol>\n",
    "    <li>Confusion Matrix</li>\n",
    "    <li>Accuracy</li>\n",
    "    <li>Precision</li>\n",
    "     <li>Recall score</li>\n",
    "    <li>F-1 score</li>\n",
    "</ol>\n",
    "<b>Confusion Matrix</b> is used here to summarise the prediction results such that the number of correct and incorrect predictions are summarized with count values and broken down by each class. This way, it is easy to visually examine how the model makes predictions<br>\n",
    "<b>Accuracy</b> is used because it shows the proportion of correct predictions to the total number of input samples. however, this does not provide enough information to make this decision<br>\n",
    "<b>Precision</b> is also selected because it tells us what proportion of predicted class is truly as predicted. The precision metric helps us to be sure of our prediction<br>\n",
    "<b>Recall</b> intuitively shows us the ability of the model to find all the positive samples. A low recall indicates many False Negatives.<br>\n",
    "<b>F1 Score</b> tells us how precise your classifier is (how many instances it classifies correctly), with respect to how robust it is (it does not miss a significant number of instances). With high precision but low recall, you classifier is extremely accurate, but it misses a significant number of instances that are difficult to classify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Evaluation Function\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "def evaluate (model):\n",
    "    \"\"\"This function is defined to evaluate a model and print the confusion matrix, accuracy, precision, recall and F-1 score\"\"\"\n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)  #integer encoded y_pred values are used for evaluation computation\n",
    "    Y_test = np.argmax(y_test, axis=1)  #integer encoded y_pred values are used for evaluation computation\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=False)   #calculates the accuracy in percentage using output test and prediction values \n",
    "    precision= precision_score(Y_test, y_pred , average=\"weighted\")*100   #calculates the weighted precision in percentage using output test and prediction values \n",
    "    recall=recall_score(Y_test, y_pred , average=\"weighted\")*100   #calculates the weighted recall in percentage using output test and prediction values \n",
    "    f1score= f1_score(Y_test, y_pred , average=\"weighted\")*100   #calculates the weighted f1-score in percentage using output test and prediction values \n",
    "    Confusion_matrix= confusion_matrix(Y_test, y_pred )        #creates confusion matrix\n",
    "    \n",
    "    # Print accuracy, f1, precision, and recall scores\n",
    "    print(\"Evaluation metrics for the model: \")\n",
    "    print('')\n",
    "    print('Confusion matrix= ')\n",
    "    print(Confusion_matrix)\n",
    "    print('')\n",
    "    print(\"Testing Accuracy= {:.2f}%\".format(accuracy*100))\n",
    "    print(\"Precision Score= {:.2f}%\".format(precision))\n",
    "    print(\"Recall= {:.2f}%\".format(recall))\n",
    "    print(\"F-1 Score= {:.2f}%\".format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for the model: \n",
      "\n",
      "Confusion matrix= \n",
      "[[ 32  71  94]\n",
      " [ 22 137  45]\n",
      " [ 28  44 127]]\n",
      "\n",
      "Testing Accuracy= 70.50%\n",
      "Precision Score= 47.13%\n",
      "Recall= 49.33%\n",
      "F-1 Score= 46.08%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model_1\n",
    "evaluate(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for the model: \n",
      "\n",
      "Confusion matrix= \n",
      "[[ 22  82  93]\n",
      " [  9 148  47]\n",
      " [ 19  52 128]]\n",
      "\n",
      "Testing Accuracy= 69.33%\n",
      "Precision Score= 48.13%\n",
      "Recall= 49.67%\n",
      "F-1 Score= 44.74%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model_2\n",
    "evaluate(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for the model: \n",
      "\n",
      "Confusion matrix= \n",
      "[[ 77  58  62]\n",
      " [ 43 134  27]\n",
      " [ 56  47  96]]\n",
      "\n",
      "Testing Accuracy= 70.39%\n",
      "Precision Score= 50.64%\n",
      "Recall= 51.17%\n",
      "F-1 Score= 50.71%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model_3\n",
    "evaluate (model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for the model: \n",
      "\n",
      "Confusion matrix= \n",
      "[[ 44  75  78]\n",
      " [ 17 145  42]\n",
      " [ 25  49 125]]\n",
      "\n",
      "Testing Accuracy= 69.67%\n",
      "Precision Score= 52.05%\n",
      "Recall= 52.33%\n",
      "F-1 Score= 49.73%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model_4\n",
    "evaluate (model_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation sumary and discussion\n",
    "The table below summarises the evaluation of the models using Accuracy, Precision, Recall and F-1:\n",
    "\n",
    "| CNN Model          | Num of filters |Accuracy(%)  |Precision(%)| Recall(%)   |F-1 score(%)|\n",
    "|--------------------|----------------|-------------|------------|-------------|------------|\n",
    "| model_1            |    100         |   70.50     | 47.13      | 49.33       | 46.08      |\n",
    "| model_2            |    150         | 69.33       | 48.13      | 49.67       | 44.74      |\n",
    "| model_3            |    200         |  70.39      | 50.64      | 51.17       | 50.71      |\n",
    "| model_4            |    250         |  69.67      | 52.05      | 52.33       | 49.73      |  \n",
    "\n",
    "1. Confusion matrix: The table below show an interpretation of the confusion matrix.\n",
    "\n",
    "\n",
    "|                    |CAG predictions| NAG predictions |OAG predictions|\n",
    "|--------------------|-------------|-------------------|---------------|\n",
    "| Actual CAG         | True CAG    |   False NAG       |  False OAG    |\n",
    "| Actual NAG         | False CAG   |   True NAG        |  False OAG    | \n",
    "| Actual OAG         | False CAG   |   False NAG       |  True OAG     | \n",
    "\n",
    "As can be seen from the matrix of every model, most of the models performed poorly at predicting Covertly aggressive (CAG) text. model_3 however performs significantly better than the rest with 77 true predictions. This means that when the aggressiveness of a text is hidden, model_3 can best predict it. NAG prediction and not really needful because a false prediction will not really cause much harm so can be ignored. with OAG prediction, model_2 makes the most correct predictions having 128 true predictions. This is especially important because not being able to detect an overtly aggressive text can be detrimental. in this case, model_3 performs the poorest.\n",
    "\n",
    "2. Accuracy:\n",
    "model_1 shows the highest accuracy meaning that it is able to make the most true predictions however, this is not enough to make a decision.\n",
    "\n",
    "3. Precision: \n",
    "Based on precision, model_4 is the most precise. A trend can be observed here as precision is seen to increases with increasing number of filters.\n",
    "\n",
    "4. Recall: \n",
    "Similar to precision, model_4 was found to have the highest recall score. A trend can also be observed here as recall is seen to increases with increasing number of filters.\n",
    "\n",
    "5. F1 Score: \n",
    "Based on F1 score, model_3 performs the least and model_3 performs best. This means to say that for a model with a good balance of pecision and recall, model_3 should be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In conclusion, the choice of a suitable model will depend on interest. With special interest in predicting covertly aggressive text, the confusion matrix shows that model_3 will perform best. If we are more intrested in detecting overtly aggressive text, model_2 will be the best fit. Accuracy and recall were seen to increase with increasing number of filters however, for a balance of accuracy and recall, model_3 perfomed best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
